---
title: "Ptt Scraper & TF-IDF"
author: "B06208030 何承諭"
date: "2019年7月19日"
output: html_document
---

> 環境建置

```{r warning=FALSE, message=FALSE}
rm(list=ls())
library(rvest)
library(tm)
library(jiebaR)
library(Matrix)
library(ggplot2)
library(varhandle)
library(knitr)
```

> 爬蟲

```{r}

links <- read_html('https://www.ptt.cc/bbs/LoL/index10004.html') %>%
  html_nodes('div.title a') %>%
  html_attrs %>%
  as.character


content <- c()
front <- "https://www.ptt.cc"
for(i in 1:length(links)){
  url <- paste0(front, links[i])
  print(url)
  
  content[i] <- read_html(url) %>% 
    html_nodes("#main-content") %>%
    html_text
}

```

> 資料清洗

```{r warning=FALSE}
content <- as.list(content)
d.corpus <- Corpus(VectorSource(content)) %>% # Corpus(VectorSource())的input是list
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(function(word) { # Regular Expression 把英文&數字的內容拿掉
    gsub("[A-Za-z0-9]", "", word)
  })
```

> 開始斷詞

```{r}

mixseg = worker()
jieba_tokenizer = function(d){ # 寫function來處理斷詞
  unlist(segment(d[[1]], mixseg))
}
seg <- lapply(d.corpus, jieba_tokenizer) # 對每一個文本(在這裡是每一個網頁內文)執行斷詞函數
n <- length(seg) # n為文本數 之後會一直用到

```

> 開始做TDM (TermDocumentMatrix)

```{r}
count_token = function(d){ # 寫function來把清單轉為dataframe
  as.data.frame(table(d))
}
tokens = lapply(seg, count_token) # lapply對list中每一個文本做出來的斷詞們轉為dataframe

TDM = tokens[[1]] # 設置一個初始TDM來方便Merge
for(id in 2:n){ # 用迴圈把list裡的所有dataframe merge起來
  TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
  names(TDM) = c('d', 1:id) # 這裡不一樣是因為我不是讀本機檔 
}

TDM[is.na(TDM)] <- 0 # 缺漏值補 0
kable(head(TDM))
```

> TF-IDF

```{r}
# apply(要做的部分, 1對Row 2對Col, 要執行什麼函數)
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum) # 等等在算詞頻時會用到
idfCal <- function(word_doc){ # idf 計算的函數 總文本數/出現該詞的文本數
  log2( n / nnzero(word_doc) ) 
}

idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal) # 對每個 Row 算 idf (有幾Col/幾個不是0)
doc.tfidf <- TDM

# 把剛剛算的tf重複擺在矩陣的Row 有幾個詞擺幾個Row
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf)) 
# 把剛剛算的idf擺在矩陣中，每個詞彙有自己獨立的idf
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
# 帥氣的把全部一起算 (每個欄位都算到 (該文本出現該詞彙的次數 / 該文本的詞彙數，也就是TF) * 已經算好的idf)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX

stopLine = rowSums(doc.tfidf[,2:(n+1)]) # tfidf 依照Row加總
delID = which(stopLine == 0) # 找到第幾個詞的RowSum是0
kable(head(doc.tfidf[delID,1])) # 這些就是氾濫字眼 (TF-IDF=0 也就是中央空調的部分log(1) = 0)

TDM = TDM[-delID,] # 不要氾濫字眼就拿掉
doc.tfidf = doc.tfidf[-delID,] # 拿掉
```

> 找重要的關鍵字

```{r}
TopWords = data.frame()
for( id in 1:n ){
  dayMax = order(doc.tfidf[,id+1], decreasing = TRUE) # 找到每一個文本 按照tf-idf大小排序
  showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1])) # 轉置這個dataframe並且取出前5高的
  TopWords = rbind(TopWords, showResult) # 跟其他Dataframe合併
}

rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords, row.names = T)

```

> 找榜上最有名的詞

```{r}
TDM$d = as.character(TDM$d)
AllTop = as.data.frame( table(as.matrix(TopWords)) ) # table來總計出現數量
AllTop = AllTop[order(AllTop$Freq, decreasing = TRUE),]

kable(head(AllTop))

```

> 製作畫圖所需的資料框

```{r}

TopNo = 5
tempGraph = data.frame()
for( t in 1:TopNo ){
  
  word <- AllTop$Var1[t] %>%
    as.matrix() %>%
    as.vector() %>%
    rep(each = n) %>%
    matrix(nrow = n)
  
  temp <- cbind( colnames(doc.tfidf)[2:(n+1)], t(TDM[which(TDM$d == AllTop$Var1[t]), 2:(n+1)]), word )
  colnames(temp) <- c("post", "freq", "words")
  tempGraph = rbind(tempGraph, temp)
  names(tempGraph) = c("post", "freq", "words")
}

```

> 畫畫

```{r}
tempGraph$freq = unfactor(tempGraph$freq)
levels(tempGraph$post) <- c(1:n) %>% as.character()

ggplot(tempGraph, aes(post, freq)) + 
  geom_point(aes(color = words, shape = words), size = 5) +
  geom_line(aes(group = words, linetype = words))
```



